(window.webpackJsonp=window.webpackJsonp||[]).push([[21],{375:function(t,s,a){"use strict";a.r(s);var n=a(45),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"transformers"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#transformers"}},[t._v("#")]),t._v(" Transformers")]),t._v(" "),a("h2",{attrs:{id:"快速开始"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#快速开始"}},[t._v("#")]),t._v(" 快速开始")]),t._v(" "),a("h3",{attrs:{id:"设计哲学"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#设计哲学"}},[t._v("#")]),t._v(" 设计哲学")]),t._v(" "),a("p",[t._v("主要设计目标")]),t._v(" "),a("ul",[a("li",[t._v("简单、快速的使用\n"),a("ul",[a("li",[t._v("每个模型只有 3 个类：configuration, models and tokenizer")]),t._v(" "),a("li",[t._v("所有类的实例化方法都是 "),a("code",[t._v("from_pretrained()")])])])]),t._v(" "),a("li",[t._v("提供和原模型在性能上无限接近的模型\n"),a("ul",[a("li",[t._v("提供例子、结果")]),t._v(" "),a("li",[t._v("接近原始代码")])])])]),t._v(" "),a("p",[t._v("其他目标")]),t._v(" "),a("ul",[a("li",[t._v("始终暴露模型的内部信息\n"),a("ul",[a("li",[t._v("使用 API 来访问隐藏层状态和 attention 参数")]),t._v(" "),a("li",[t._v("tokenizer 和 base 模型的 API 可以随意在其他模型间切换")]),t._v(" "),a("li",[t._v("添加新 tokens 的方法简单")]),t._v(" "),a("li",[t._v("为 transformer 的 heads 剪枝的方法简单")])])])]),t._v(" "),a("h3",{attrs:{id:"核心概念"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#核心概念"}},[t._v("#")]),t._v(" 核心概念")]),t._v(" "),a("ul",[a("li",[t._v("model classes：PyTorch 或 TF 模型，例如 "),a("code",[t._v("BertModel")])]),t._v(" "),a("li",[t._v("configuration classes：存储模型所需的所有参数，例如 "),a("code",[t._v("BertConfig")]),t._v("，pretrain 的模型都有默认参数")]),t._v(" "),a("li",[t._v("tokenizer classes：每个模型中存储的词汇，并提供对 embedding 进行 encoding/decoding 成字符串的方法，embedding 可直接输入到模型中使用。")])]),t._v(" "),a("p",[t._v("所有这些类的实例化方法")]),t._v(" "),a("ul",[a("li",[a("code",[t._v("from_pretrained()")]),t._v("：实例化 pretrain 版本的 model/configuration/tokenizer")]),t._v(" "),a("li",[a("code",[t._v("save_pretrained()")]),t._v("：在本地保存 model/configuration/tokenizer，之后便可被 "),a("code",[t._v("from_pretrained()")]),t._v(" 加载")])]),t._v(" "),a("h3",{attrs:{id:"bert-指南"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bert-指南"}},[t._v("#")]),t._v(" BERT 指南")]),t._v(" "),a("p",[t._v("步骤")]),t._v(" "),a("ol",[a("li",[t._v("使用 "),a("code",[t._v("BertTokenizer")]),t._v(" 将文本 tokenize 化（可直接输入到模型的 token embedding 数组）\n"),a("ol",[a("li",[a("code",[t._v("convert_tokens_to_ids(tokenized_text)")]),t._v("：将文本转换为词表里 token 的索引，转换后可将其输入模型")]),t._v(" "),a("li",[a("code",[t._v("convert_ids_to_tokens([predict_index])")]),t._v("：将索引信息转换为 text 信息")])])]),t._v(" "),a("li",[t._v("将 embedding 索引输入到模型做预测，可以使用模型（"),a("code",[t._v("BertModel")]),t._v("）获得最后一层的隐藏信息，也可以使用其他模型（"),a("code",[t._v("BertForMaskedLM")]),t._v("）做 Mask 预测\n"),a("ol",[a("li",[a("code",[t._v("model.eval()")]),t._v(" 预测时去除 DropOut 模块，这样预测结果是可重入的")])])])]),t._v(" "),a("h2",{attrs:{id:"术语表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#术语表"}},[t._v("#")]),t._v(" 术语表")]),t._v(" "),a("h3",{attrs:{id:"input-ids"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#input-ids"}},[t._v("#")]),t._v(" Input IDs")]),t._v(" "),a("p",[t._v("id 通常是输入到模型中的唯一参数")]),t._v(" "),a("ul",[a("li",[t._v("token indices：token 对应的数字索引")])]),t._v(" "),a("h3",{attrs:{id:"attention-mask"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#attention-mask"}},[t._v("#")]),t._v(" Attention mask")]),t._v(" "),a("p",[t._v("它是一个可选的参数，告诉模型哪些 id 需要关注，哪些不需要；在 "),a("code",[t._v("BertTokenizer")]),t._v(" 中，1 表示需要关注，0 表示不需要。")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" BertTokenizer\n\ntokenizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BertTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bert-base-cased"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsequence_a "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This is a short sequence."')]),t._v("\nsequence_b "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This is a rather long sequence. It is at least longer than the sequence A."')]),t._v("\n\nsequence_a_dict "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encode_plus"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sequence_a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" max_length"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("19")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pad_to_max_length"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" sequence_a_dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'input_ids'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("101")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1188")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1110")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("170")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1603")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4954")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("119")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("102")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" sequence_a_dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'attention_mask'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),a("h3",{attrs:{id:"token-type-ids"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#token-type-ids"}},[t._v("#")]),t._v(" Token Type IDs")]),t._v(" "),a("p",[t._v("需要 2 个 sequences 输入的场景（sequence classification or question answering），这 2 个 sequences 需要被特殊符号分隔")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("[CLS] SEQ_A [SEP] SEQ_B [SEP]\n")])])]),a("p",[t._v("使用 "),a("code",[t._v("encode()")]),t._v(" 可对 2 个 sequences 进行处理")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("sequence_a "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"HuggingFace is based in NYC"')]),t._v("\nsequence_b "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Where is HuggingFace based?"')]),t._v("\n\nencoded_sequence "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encode"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sequence_a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" sequence_b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decode"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("encoded_sequence"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]"')]),t._v("\n")])])]),a("p",[t._v("有些模型，如 BERT 还需要 segment IDs")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("encoded_dict "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encode_plus"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sequence_a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" sequence_b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" encoded_dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'input_ids'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("101")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20164")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10932")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2271")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("7954")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1110")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1359")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1107")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("17520")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("102")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2777")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1110")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20164")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10932")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2271")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("7954")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1359")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("136")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("102")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" encoded_dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'token_type_ids'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),a("h3",{attrs:{id:"position-ids"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#position-ids"}},[t._v("#")]),t._v(" Position IDs")]),t._v(" "),a("p",[t._v("帮助模型识别每个 token 所处的位置，这是个可选的参数，如果没有 position IDs 输入到模型，模型会自动创建绝对位置的 embeddings。")]),t._v(" "),a("p",[t._v("绝对位置的 embeddings 处于 "),a("code",[t._v("[0, config.max_position_embeddings - 1]")]),t._v(" 范围中。有些模型使用其他类型的位置 embeddings，例如相对位置 embeddings 或 sinusoidal position embeddings。")]),t._v(" "),a("h2",{attrs:{id:"pretrained-models"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pretrained-models"}},[t._v("#")]),t._v(" Pretrained models")]),t._v(" "),a("p",[t._v("https://huggingface.co/models")]),t._v(" "),a("h2",{attrs:{id:"使用说明"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#使用说明"}},[t._v("#")]),t._v(" 使用说明")]),t._v(" "),a("p",[t._v("用最简单的例子对用例进行说明：sequence classificatin，question answering，name entity recognition ...")]),t._v(" "),a("p",[t._v("这些例子基于 auto-models（已经 fine-tune 的 checkpoints 模型），更多信息参照 AutoModel 文档")]),t._v(" "),a("ul",[a("li",[t._v("如果你想 fine-tune 一个模型来处理特定的任务，你可以使用 example 里的 run_$TASK.py 脚本，并在此基础上进行修改")])]),t._v(" "),a("p",[t._v("为了完成推断任务，库还提供以下机制")]),t._v(" "),a("ul",[a("li",[t._v("pipelines：简单易用的抽象")]),t._v(" "),a("li",[t._v("tokenizer + model：不那么抽象，但很强大")])]),t._v(" "),a("h3",{attrs:{id:"sequence-classification"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sequence-classification"}},[t._v("#")]),t._v(" Sequence Classification")]),t._v(" "),a("p",[t._v("数据集：https://github.com/nyu-mll/jiant/blob/master/scripts/download_glue_data.py")]),t._v(" "),a("h3",{attrs:{id:"extractive-question-answering"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#extractive-question-answering"}},[t._v("#")]),t._v(" Extractive Question Answering")]),t._v(" "),a("blockquote",[a("p",[t._v("Extractive Question Answering is the task of extracting an answer from a text given a question.")])]),t._v(" "),a("p",[t._v("数据集：SQuAD dataset")]),t._v(" "),a("p",[t._v("example: "),a("em",[t._v("un_squad.py")])]),t._v(" "),a("h3",{attrs:{id:"language-modeling"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#language-modeling"}},[t._v("#")]),t._v(" Language Modeling")]),t._v(" "),a("blockquote",[a("p",[t._v("Language modeling is the task of fitting a model to a corpus, which can be domain specific.")])]),t._v(" "),a("p",[t._v("e.g. BERT is trained with masked language modeling; GPT-2 with causal language modeling.")]),t._v(" "),a("p",[t._v("example:")]),t._v(" "),a("blockquote",[a("p",[t._v("using a language model trained over a very large corpus, and then fine-tuning it to a news dataset or on scientific papers e.g. "),a("a",{attrs:{href:"https://huggingface.co/lysandre/arxiv-nlp",target:"_blank",rel:"noopener noreferrer"}},[t._v("LysandreJik/arxiv-nlp"),a("OutboundLink")],1),t._v(".")])]),t._v(" "),a("h4",{attrs:{id:"masked-language-modeling"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#masked-language-modeling"}},[t._v("#")]),t._v(" Masked Language Modeling")]),t._v(" "),a("h4",{attrs:{id:"causal-language-modeling"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#causal-language-modeling"}},[t._v("#")]),t._v(" Causal Language Modeling")]),t._v(" "),a("h3",{attrs:{id:"named-entity-recognition"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#named-entity-recognition"}},[t._v("#")]),t._v(" Named Entity Recognition")]),t._v(" "),a("blockquote",[a("p",[t._v("Named Entity Recognition (NER) is the task of classifying tokens according to a class, for example identifying a token as a person, an organisation or a location.")])]),t._v(" "),a("p",[t._v("数据集：CoNLL-2003")]),t._v(" "),a("p",[t._v("example：ner/run_ner.py (PyTorch), ner/run_pl_ner.py (leveraging pytorch-lightning) or the ner/run_tf_ner.py (TensorFlow)")]),t._v(" "),a("h2",{attrs:{id:"notebooks"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#notebooks"}},[t._v("#")]),t._v(" Notebooks")]),t._v(" "),a("p",[t._v("用 3 个 Notebooks 来比较 PyTorch 模型和 TF 模型之间的差异")]),t._v(" "),a("ul",[a("li",[t._v("比较每一个隐藏层之间的差异")]),t._v(" "),a("li",[t._v("比较 fine-tuning 层之间的 loss 的差异")]),t._v(" "),a("li",[t._v("比较 MLM 的预测结果")])])])}),[],!1,null,null,null);s.default=e.exports}}]);