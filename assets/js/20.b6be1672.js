(window.webpackJsonp=window.webpackJsonp||[]).push([[20],{374:function(a,e,t){"use strict";t.r(e);var r=t(45),s=Object(r.a)({},(function(){var a=this,e=a.$createElement,t=a._self._c||e;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h1",{attrs:{id:"【学习笔记】hands-on-machine-learning-chap3-classification"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#【学习笔记】hands-on-machine-learning-chap3-classification"}},[a._v("#")]),a._v(" 【学习笔记】Hands On Machine Learning - Chap3. Classification")]),a._v(" "),t("p",[a._v("本章首先介绍了 MNIST 数据集，此数据集为 7 万张带标签的手写数字（0-9）图片，它被认为是机器学习领域的 HelloWorld，很多机器学习算法都可以在此数据集上进行训练、调参、对比。")]),a._v(" "),t("p",[a._v("本章核心内容在如何评估一个分类器，介绍了混淆矩阵、Precision 和 Reccall 等衡量正样本的重要指标，及如何对这两个指标进行取舍，此外，还介绍了 ROC 曲线及 AUC 值，当然，肯定少不了 F1Score 了。")]),a._v(" "),t("p",[a._v("最后，本章还介绍了构建多分类器的一般方法。作为科普，你还可以构建多 label 的分类器，以及每个 label 可取不同 value 的分类器。")]),a._v(" "),t("p",[a._v("下面是详细笔记：")]),a._v(" "),t("h2",{attrs:{id:"mnist"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#mnist"}},[a._v("#")]),a._v(" MNIST")]),a._v(" "),t("p",[a._v("MNIST 数据集：70000 张手写数字小图片。这些图片被誉为 ML 中的 Hello World。")]),a._v(" "),t("p",[a._v("手动加载 MNIST 步骤：")]),a._v(" "),t("ol",[t("li",[a._v("下载 "),t("a",{attrs:{href:"https://github.com/amplab/datascience-sp14/raw/master/lab7/mldata/mnist-original.mat",target:"_blank",rel:"noopener noreferrer"}},[a._v("mnist-original.mat"),t("OutboundLink")],1)]),a._v(" "),t("li",[a._v("调用 "),t("code",[a._v("sklearn.datasets.base.get_data_home()")]),a._v(" 查看 sklearn 下载到本地的路径")]),a._v(" "),t("li",[a._v("将下载后的文件 "),t("code",[a._v("mnist-original.mat")]),a._v(" 拷贝到 "),t("code",[a._v("get_data_home()/mldata")]),a._v(" 目录下")]),a._v(" "),t("li",[a._v("调动 "),t("code",[a._v("fetch_mmldata()")]),a._v(" 接口，获取 "),t("code",[a._v("mnist")]),a._v(" 对象：如本地有，就不会从网上下载")])]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("from sklearn.datasets.base import get_data_home \nfrom sklearn.datasets import fetch_mldata\nprint (get_data_home())\nmnist = fetch_mldata('MNIST original')\nmnist\n----\n/Users/fengyajie/scikit_learn_data\n{'DESCR': 'mldata.org dataset: mnist-original',\n 'COL_NAMES': ['label', 'data'],\n 'target': array([0., 0., 0., ..., 9., 9., 9.]),\n 'data': array([[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}\n")])])]),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v('# 查看数据，该数据中包含 70000 张图片，每张图片拥有 784 个 features，\n# 因为该图片的规格为 28x28，每个像素的值的范围是 0(white)-255(black)\nX,y = mnist["data"],mnist["target"]\nprint(X.shape,y.shape)\n----\n(70000, 784) (70000,)\n')])])]),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v('# 显示其中一个样本\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nsome_digit = X[36000]\nsome_digit_image = some_digit.reshape(28, 28)\nplt.imshow(some_digit_image, cmap = matplotlib.cm.binary,\n           interpolation="nearest")\nplt.axis("off")\nplt.show()\ny[36000]\n')])])]),t("p",[t("img",{attrs:{src:"https://upload-images.jianshu.io/upload_images/1933644-3d7bd9f7ea927484.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240",alt:""}})]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("# 测试集和训练集\nX_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n# 打散训练集，避免相似的图片都在一块\nimport numpy as np\nshuffle_index = np.random.permutation(60000)\nX_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n")])])]),t("h2",{attrs:{id:"训练二分类器"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#训练二分类器"}},[a._v("#")]),a._v(" 训练二分类器")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v('from sklearn.model_selection import cross_val_score\n# 识别数字 5 的分类器，使用 sklearn 提供的随机梯度下降算法\ny_train_5 = (y_train == 5)\ny_test_5 = (y_test == 5)\nfrom sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(random_state=42)\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")\n----\narray([0.9578 , 0.9607 , 0.96775])\n')])])]),t("h2",{attrs:{id:"效果评估"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#效果评估"}},[a._v("#")]),a._v(" 效果评估")]),a._v(" "),t("p",[a._v("上面模型的准确率很高，有一个原因是其正样本的比例只有 10%，这种情况下，即便我全部猜【不是5】，准确率也有 90% 之高，所以一般我们不用准确率来衡量模型的好坏。")]),a._v(" "),t("h3",{attrs:{id:"混淆矩阵"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#混淆矩阵"}},[a._v("#")]),a._v(" 混淆矩阵")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\nconfusion_matrix(y_train_5, y_train_pred)\n----\narray([[53556,  1023],\n       [ 1252,  4169]])\n")])])]),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v('# 输出Precision score和recall score\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import f1_score\nprint("precision_score={}, recall_score={}".format(precision_score(y_train_5, y_train_pred), recall_score(y_train_5, y_train_pred)))\nprint("f1_score={}".format(f1_score(y_train_5, y_train_pred)))\n----\nprecision_score=0.8029661016949152, recall_score=0.7690463014204021\nf1_score=0.7856402525204936\n')])])]),t("h3",{attrs:{id:"precision-recall-tradeoff"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#precision-recall-tradeoff"}},[a._v("#")]),a._v(" Precision/Recall tradeoff")]),a._v(" "),t("p",[a._v("precision 和 recall 往往不能两全，一个提升了，另一个会下降，这两个指标需要进行权衡，例如在判断视频节目是否对小孩无害的场景下，我们希望 precision 越高越好，同时可以牺牲 recall；而在根据照片预测小偷的场景下，更希望 recall 越高越好。")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v('# 绘制 precision 和 recall 曲线\nfrom sklearn.metrics import precision_recall_curve\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n                                 method="decision_function")\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds): \n    plt.plot(thresholds, precisions[:-1], "b--", label="Precision") \n    plt.plot(thresholds, recalls[:-1], "g-", label="Recall") \n    plt.xlabel("Threshold")\n    plt.legend(loc="upper left")\n    plt.ylim([0, 1])\n    \nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()\n')])])]),t("p",[t("img",{attrs:{src:"https://upload-images.jianshu.io/upload_images/1933644-4286faeae14b43e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240",alt:""}})]),a._v(" "),t("h3",{attrs:{id:"roc-曲线"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#roc-曲线"}},[a._v("#")]),a._v(" ROC 曲线")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\ndef plot_roc_curve(fpr, tpr, label=None): \n    plt.plot(fpr, tpr, linewidth=2, label=label) \n    plt.plot([0, 1], [0, 1], 'k--') \n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\nplot_roc_curve(fpr, tpr)\nplt.show()\n")])])]),t("p",[t("img",{attrs:{src:"https://upload-images.jianshu.io/upload_images/1933644-f8fa7f52bea69285.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240",alt:""}})]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("# 计算 AUC\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_train_5, y_scores)\n----\n0.9655990736206981\n")])])]),t("p",[a._v("使用 F1Score 还是 AUC？取决于正样本和负样本的比例，如果正样本较少，你应该选择 F1Score，否则选择 AUC。")]),a._v(" "),t("p",[a._v("使用随机森林")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v('from sklearn.ensemble import RandomForestClassifier\nforest_clf = RandomForestClassifier(random_state=42)\ny_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n                                    method="predict_proba")\ny_scores_forest = y_probas_forest[:, 1]\nfpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)\n\nplt.plot(fpr, tpr, "b:", label="SGD")\nplot_roc_curve(fpr_forest, tpr_forest, "Random Forest")\nplt.legend(loc="bottom right")\nplt.show()\n')])])]),t("p",[t("img",{attrs:{src:"https://upload-images.jianshu.io/upload_images/1933644-d6f9ee86418accdf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240",alt:""}})]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("# 随机森林的 auc\nroc_auc_score(y_train_5, y_scores_forest)\n----\n0.993283808868663\n")])])]),t("h2",{attrs:{id:"多分类器"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#多分类器"}},[a._v("#")]),a._v(" 多分类器")]),a._v(" "),t("p",[a._v("分类器的分类")]),a._v(" "),t("ol",[t("li",[a._v("二分类器：Logistic Regression、SVM")]),a._v(" "),t("li",[a._v("多分类器：Random Forest、Naive Bayes")])]),a._v(" "),t("p",[a._v("除此之外，你也可以使用二分类器来构造多分类器，例如识别 0-9 十个数字，你可以训练 10 个二分类器，每个分类器用来识别一个数字，当你要预测一个数字时，将该数字分别输入到这十个分类器中，最后获得最高分的那个分类器，就是预测结果。这种方法也被称为 one-versus-all (OvA)")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("# 在 sklearn 中，其内部会自动训练多个分类器，并且在预测时给出分数最高的那个分类\nsgd_clf.fit(X_train, y_train)\nsgd_clf.predict([some_digit])\n----\narray([5.])\n")])])]),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("some_digit_scores = sgd_clf.decision_function([some_digit])\nsome_digit_scores\n----\narray([[-227250.7532523 , -511911.42747987, -343850.9936749 ,\n        -194518.44134798, -341796.12282028,   10728.59041333,\n        -798149.80620821, -263564.01751255, -729498.66535121,\n        -553349.11568488]])\n")])])]),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("# 最高分数的下标\nnp.argmax(some_digit_scores)\n# 分类\nsgd_clf.classes_\nsgd_clf.classes_[5]\n----\n5.0\n")])])]),t("h2",{attrs:{id:"错误分析"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#错误分析"}},[a._v("#")]),a._v(" 错误分析")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("# 交叉验证 + 混淆矩阵\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n\ny_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\nconf_mx = confusion_matrix(y_train, y_train_pred)\nconf_mx\n----\narray([[5757,    4,   17,   11,    9,   38,   35,   11,   38,    3],\n       [   2, 6465,   47,   23,    6,   45,    7,   13,  121,   13],\n       [  59,   42, 5330,   96,   91,   24,   81,   55,  163,   17],\n       [  45,   44,  139, 5352,    0,  227,   35,   56,  130,  103],\n       [  22,   26,   37,    8, 5360,    7,   46,   34,   74,  228],\n       [  88,   42,   31,  196,   81, 4577,  107,   28,  175,   96],\n       [  40,   25,   48,    2,   44,   86, 5616,    9,   48,    0],\n       [  24,   19,   69,   32,   58,   10,    4, 5785,   17,  247],\n       [  57,  148,   79,  149,   11,  162,   56,   24, 5003,  162],\n       [  45,   35,   27,   86,  161,   29,    2,  182,   67, 5315]])\n")])])]),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("# 使用图像来表示混淆矩阵\nplt.matshow(conf_mx, cmap=plt.cm.gray)\nplt.show()\n")])])]),t("p",[t("img",{attrs:{src:"https://upload-images.jianshu.io/upload_images/1933644-97ba00e5479864dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240",alt:""}})]),a._v(" "),t("p",[a._v("因为所有的图像都在主对角线上，所有该混淆矩阵看上去不错，5 号分类器看上去颜色深一点，说明它的预测效果没有其他分类器好")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("# 查看错误率，row_sums 是每个分类中实际的样本数\nrow_sums = conf_mx.sum(axis=1, keepdims=True)\nnorm_conf_mx = conf_mx/row_sums \nnp.fill_diagonal(norm_conf_mx, 0) # 填充对角线，只留出错误的数据\nplt.matshow(norm_conf_mx, cmap=plt.cm.gray)\nplt.show()\n")])])]),t("p",[t("img",{attrs:{src:"https://upload-images.jianshu.io/upload_images/1933644-b45bf8a9a3d9a0f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240",alt:""}})]),a._v(" "),t("p",[a._v("8 和 9 两列比较白，意味着很多数字都错误的分类到了 8 和 9 两个数上；颜色非常深的行，意味着这个数字基本上预测对了，例如 1；")]),a._v(" "),t("p",[a._v("对于你想调优的分类器，你可以相应的增加样本；或优化样本图片（使用 Scikit-Image, Pillow, or OpenCV），例如使它们都处于图片正中间，且不要过于偏斜。")]),a._v(" "),t("h2",{attrs:{id:"多个-label-的分类器"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#多个-label-的分类器"}},[a._v("#")]),a._v(" 多个 label 的分类器")]),a._v(" "),t("p",[a._v("向分类器输入一组数据，它会输出多个预测值，例如下面的程序，可以同时预测图片是否是大数（>=7）及图片是否是奇数")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("from sklearn.neighbors import KNeighborsClassifier\ny_train_large = (y_train >= 7)\ny_train_odd = (y_train % 2 == 1)\ny_multilabel = np.c_[y_train_large, y_train_odd]\n\n# Kneighbors 分类器可以同时输出多组预测值\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train, y_multilabel)\nknn_clf.predict([some_digit])\n----\narray([[False,  True]])\n")])])]),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v('# 你可以使用 f1_score + 交叉验证 的方法来衡量多值分类器的效果\n# 如果大数的图片远远多于奇数的图片，你可以将对每个label赋予一个权重，权重值根据其值的占比来设定\n#  方法也很简单，将下面的参数 average 设为 average="weighted" 即可\ny_train_knn_pred = cross_val_predict(knn_clf, X_train, y_train, cv=3)\nf1_score(y_train, y_train_knn_pred, average="macro")\n')])])]),t("h2",{attrs:{id:"多输出的分类器"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#多输出的分类器"}},[a._v("#")]),a._v(" 多输出的分类器")]),a._v(" "),t("p",[a._v("将多 label 的分类器进行扩展，每个 label 不止是 2 个值的分类器为多输出的分类器，下面的例子是：")]),a._v(" "),t("p",[a._v("特征为带有噪音的图片（每个像素在原有图片的基础上加入噪声），target 为无噪音的图片，预测输出一张没有噪音的图片，即图片的每个像素为 1 个 label，每个 label 的取值范围为 0-255")]),a._v(" "),t("p",[a._v("以上是该书第三章的学习笔记，你也可以下载 "),t("a",{attrs:{href:"https://github.com/jieniu/HOML-exercises/blob/master/chapter3/ch3_note.ipynb",target:"_blank",rel:"noopener noreferrer"}},[a._v("Jupyter NoteBook"),t("OutboundLink")],1),a._v(" 来具体操练一下。")]),a._v(" "),t("p",[a._v("相关文章：")]),a._v(" "),t("ul",[t("li",[t("RouterLink",{attrs:{to:"/AI/homl-ch2.html"}},[a._v("【学习笔记】Hands On Machine Learning - Chap2. End-to-End Machine Learning Project")])],1)])])}),[],!1,null,null,null);e.default=s.exports}}]);